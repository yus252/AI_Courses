{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 12: Exploring Unfairness in Data ⚖️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* Understanding and Applying Linear Regression \n",
    "* Data Exploration\n",
    "* Practice ML Workflow: Training, Testing, and Evaluation\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. [Unfairness](#1.-Unfairness)\n",
    "2. [Exploring Loan Approval Data](#2.-Exploring-Loan-Approval-Data)\n",
    "3. [Building a Model](#3.-Building-a-Model)\n",
    "4. [Becoming Data and Fairness Aware](#4.-Becoming-Data-and-Fairness-Aware)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Unfairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is natural to assume that a model built from \"real-world\" data will inherently represent the world-at-large. We often take the data that we have for granted, especially when we are first getting started with Data Science. However, if we do not pay attention to what our data look like, how they were collected, and what features they contain, we may unknowingly create models that propagate cultural biases and unfairnesses.\n",
    "\n",
    "![hire](utility/images/undraw_hire_te5y.png)\n",
    "\n",
    "In 2014, Amazon began building programs that could automate the hiring process for engineers. They wanted a machine to be able to pick out the top resumes from the thousands they receive every year. They trained their model on all of the resumes that they had, hoping that the model would be able to identify trends in keyword frequency within those applications. If most applications contained the word \"intern,\" then one might reasonably expect that a resume containing it would be ranked higher than one that doesn't. However, as they began to deploy their model, it became increasingly apparent that the model was discriminating against women. When engineers investigated why this was the case, they found that the data they trained the model with, the resumes, had mostly come from men. The model had learned to prefer resumes that didn't contain the word \"women's\" because that word wasn't frequent seen during its training. Although gender was not explicitly a feature of the dataset, it was still present in the dataset, encoded within the experiences that applicants reported. \n",
    "\n",
    "Amazon's case serves as a reminder that we must be careful of our data, even more so today as data becomes cheaper to collect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring Loan Approval Data\n",
    "\n",
    "Imagine that you are a data scientist at a bank and that one of your company's primary business areas is in lending money. The current loan approval process, that has been in place since the founding of the bank, has always relied on manual review of applications -- a process that is tedious and doesn't scale well in the modern age. The company wants to expand their business, but this archeic system is holding them back.\n",
    "\n",
    "Think about how to approach this problem, you immediately think of using the bank's past loan approval records to build a model that can learn how a human application reviewer decides which applications to approve and which to reject.\n",
    "\n",
    "![approval](utility/images/undraw_accept_request_vdsd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquiring the Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let's make sure that we have the data. The cell below checks if you have the `loan-payments.csv` file in the `utility/data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "\n",
    "\n",
    "data_dir = 'utility/data'\n",
    "\n",
    "assert exists(f'{data_dir}/loan-payments.csv'), 'Loan data file is missing.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load our data. In the cell below, we read our [CSV][1] file into a [Pandas][2] [`DataFrame`][3] called `data`.\n",
    "\n",
    "\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Comma-separated_values\n",
    "[2]: https://pandas.pydata.org/\n",
    "[3]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(f'{data_dir}/loan-payments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write-up!** How many examples are in our data set? How many features does it have?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write-up!** With your neighbor, come up with a description of what you think each feature is and what type of feature each one is. Which one should be our target variable? Which ones do you think will be useful for our model?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Some Adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's drop the columns in `data` that contain features that we are not interested in. Since `loan_id`s are not informative for predicting new loans, we can ignore them. Additionally, `effective_date`, `due_date`, and `paid_time_off` are all encoded in `past_due_days`. It is unlikely that the specifics of when a loan was due is predictive of success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_interested = ['loan_id', 'effective_date', 'due_date', 'paid_off_time']\n",
    "\n",
    "data = data.drop(not_interested, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that `past_due_days` has `NaN` values?\n",
    "\n",
    "**Write-up!** Why might some of the values in `past_due_days` be `NaN`?  With your neighbor, discuss what we should do about these values and note your conclusion below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Because this person has paid before the due day.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Replace the values in `past_due_days` with a reasonable value. `HINT` you can use the `fillna` function on `DataFrame`s to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "data['past_due_days'] = data['past_due_days'].fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Data Set\n",
    "\n",
    "Now that we have narrowed down the features we want to use, let's visualize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** For each feature, make a new cell below and create a plot that we can use to understand the values of that feature. These plots should be appropriate for the type of each feature (e.g. use a bar plot for categorical features). Ensure that you have all the components off a nice plot, making sure to include things like axes labels, a legend, and a title. Also include a `raw` cell below each, describing what you see. `HINT` you can copy and paste groups of cells by shift-clicking them on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python library matplotlib\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# this cell is free!\n",
    "\n",
    "# your code here\n",
    "\n",
    "# Bar plot for Age\n",
    "ages = data['age'] \n",
    "bins = [10,20,30,40,50,60]\n",
    "\n",
    "plt.hist(ages, bins, histtype='bar', rwidth=0.8)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Feature: Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for Education\n",
    "data['education'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gender'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Use \"Raw\" cells to describe what you see\n",
    "\n",
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Model\n",
    "\n",
    "Now that we have a sense for the nuances of our dataset we can try building some models.\n",
    "\n",
    "![analytics](utility/images/undraw_predictive_analytics_kf9n.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, we will need to encode our categorical features with enumerations instead of the string values that they currently have. As a reminder, this is what our dataset looks like right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easy way to do this encoding is to use the [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) from `sklearn`. In the cell below, we create a list called `categorical` containing the names of the columns corresponding to the categorical features in our dataset. We then create and instance of a `LabelEncoder` and use it to transform the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn library\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical = ['loan_status', 'education', 'gender']\n",
    "\n",
    "# create an instance of a LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# make a copy of our data\n",
    "encoded = data.copy()\n",
    "\n",
    "# apply the encoder's `fit_transform` method to the values for each categorical\n",
    "# feature column\n",
    "encoded[categorical] = data[categorical].apply(encoder.fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the categorical values like \"PAIDOFF\" have now been replaced with numbers. We can see which numbers map to each value like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in categorical:\n",
    "    print(*sorted(zip(encoded[column].unique(), data[column].unique()), key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's separate our features from our target variable, `loan_status`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = encoded.loc[:, encoded.columns != 'loan_status'], encoded.loan_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing a Baseline\n",
    "\n",
    "Now we're ready to start building models. First, let's create a train/test split of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's train and evaluate a LogisticRegression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** In the cell below, evaluate the model's performance on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "print(f'''\n",
    "validation score: {model.score(X_test, y_test)}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write-up!** How does our model perform on the test set?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also try looking at the model's performance on test examples of different genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''\n",
    "validation (men) score: {model.score(X_test[X_test['gender'] == 1], y_test[X_test['gender'] == 1]):0.3f}\n",
    "validation (women) score: {model.score(X_test[X_test['gender'] == 0], y_test[X_test['gender'] == 0]):0.3f}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes!\n",
    "\n",
    "**Write-up!** What do you notice about these scores? How does these compare with the initial score we saw for the entire test set? What does this imply about our model?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Gender\n",
    "\n",
    "So our model is biased with respect to gender and gender is a feature of the model. Would it help to ignore the gender feature during training? Let's try it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating another train/test split, but this time using a copy of `X` and `y` that don't include `gender`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_without_gender = X.drop(['gender'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X_without_gender, y, test_size=0.2, stratify=y, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what `X_train` looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's repeat our procedure for our baseline experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f'''\n",
    "\n",
    "validation score: {model.score(X_test, y_test)}\n",
    "validation (men) score: {model.score(X_test[X.iloc[X_test.index]['gender'] == 1],\n",
    "                                     y_test[X.iloc[X_test.index]['gender'] == 1]):0.3f}\n",
    "validation (women) score: {model.score(X_test[X.iloc[X_test.index]['gender'] == 0],\n",
    "                                       y_test[X.iloc[X_test.index]['gender'] == 0]):0.3f}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the same?\n",
    "\n",
    "**Write-up!** With your neighbor, discuss what this might imply about our model and our data. Also, discuss why it may or may not be a good idea to ignore \"protected variables\" like \"gender\" when training a model. Record your response below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Becoming Data and Fairness Aware\n",
    "\n",
    "![team](utility/images/undraw_team_spirit_hrr4.png)\n",
    "\n",
    "The goal of today's lab was to demonstrate how an accuracy score can mislead you into thinking that your model is great and that your mission has been accomplished. By digging only a little bit deeper and evaluating our model's performance on each gender separately, we found that it performed very differently between genders. It was biased!\n",
    "\n",
    "Just like that, while we were building a model to predict creditworthiness and loan repayment, we ran into the same problem Amazon did with their resume reviewing algorithm. Because the data we used was imbalanced, we introduced bias into our model unintentionally.\n",
    "\n",
    "If you search online, you will find a myriad of ways that could be used to mitigate the effects of an imbalanced dataset. However, at the end of the day, the **best solution for both Amazon and us is to collect more complete data**.\n",
    "\n",
    "We have only barely scratched the surface of fairness in Data Science. The field is both complex and emerging. If you are looking for more information about, I recommend starting with [Google's overview](https://developers.google.com/machine-learning/fairness-overview/) of the topic.\n",
    "\n",
    "I hope that you will leave here today with a different, more careful perspective on your data and how it might unintentionally create bias in your models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
